
If in gradient descent, if J (w ->, b) is static / moving up and down, It is a clear sign that gradient descent is not working.  

Cause may be : Bug In code or a learning rate (alpha) that is to large

To fix, use a smaller a or learning rate 

with a small enough a, J(w->, b) should decrease on every iteration, make sure that w1= w1 - ad1 not the opposite

Feature Engineering : Using intuition to design new features, by transforming or combining original features. 

example : [house] -- area = x1 (frontage) * x2 (depth)

	which then creates feature x3 = area. Can be used in the model for more training 





